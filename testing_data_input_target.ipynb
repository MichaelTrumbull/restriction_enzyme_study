{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4616dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a80ae013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    CGANNNNNNNRTAG/CTAYNNNNNNNTCG\n",
      "Name: Methylation Motif, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"raw_data/metadata_600.csv\")\n",
    "for i in range(600):\n",
    "    #print(protien_names_ordered_filtered[i][3])\n",
    "    pass\n",
    "\n",
    "print(df.loc[df['RMS_Name'] == 'Aap10433IP']['Methylation Motif'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d3661eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCANNNNNNRTAC/GTAYNNNNNNTGG'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df['RMS_Name'] == 'Aap5906IP']['Methylation Motif'].tolist()[0]\n",
    "# find the location where RMS_Name is Aap10433IP. \n",
    "# Then get what is in Methylation Motif colum. \n",
    "# This is a list of length 1. So just use [0] to get the value we want.\n",
    "\n",
    "# make list of strings that is sorted accordingly to protien_names_ordered_filtered[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7904e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "MethylationMotif = []\n",
    "for i in range(600):\n",
    "    MethylationMotif.append( df.loc[df['RMS_Name'] == protien_names_ordered_filtered[i][3]]['Methylation Motif'].tolist()[0] )\n",
    "print(len(MethylationMotif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d63d1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas4_dict = {\n",
    "    'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1],\n",
    "    'R': [1,0,1,0], 'Y': [0,1,0,1], 'K': [0,0,1,1], 'M': [1,1,0,0], 'S': [0,1,1,0], 'W': [1,0,0,1],\n",
    "    'B': [0,1,1,1], 'D': [1,0,1,1], 'H': [1,1,0,1], 'V': [1,1,1,0], 'N': [1,1,1,1] \n",
    "}\n",
    "def str_to_base4code(string):\n",
    "    coded = []\n",
    "    for char in string:\n",
    "        coded = coded + bas4_dict[char]\n",
    "    return coded\n",
    "# functions for building list of encoded strings and padding the lists\n",
    "def one_hot(data_1d):\n",
    "    data_1d_encoded = []\n",
    "    for string in data_1d:\n",
    "        data_1d_encoded.append(str_to_base4code(string))\n",
    "    return data_1d_encoded\n",
    "def pad(data_1d):\n",
    "    max_length = len(max(data_1d, key=len))\n",
    "    padded_data = []\n",
    "    for i, line in enumerate(data_1d):\n",
    "        padded_data.append(line + [0]*(max_length - len(line)))\n",
    "    return padded_data\n",
    "df = pd.read_csv(\"raw_data/metadata_600.csv\")\n",
    "\n",
    "# we will split the data by \"left / right\" Data: CGANNNNNNNRTAG/CTAYNNNNNNNTCG\n",
    "MethylationMotif = []\n",
    "for i in range(600):\n",
    "    MethylationMotif.append( df.loc[df['RMS_Name'] == protien_names_ordered_filtered[i][3]]['Methylation Motif'].tolist()[0] )\n",
    "left_list = []\n",
    "right_list = []\n",
    "for line in MethylationMotif:\n",
    "    try: left, right = line.rsplit('/')\n",
    "    except: left, right = line.rsplit('/')[0], '' # if there is no \"/\" then we will just fill \"right\" with ''\n",
    "    left_list.append(left)\n",
    "    right_list.append(right)\n",
    "\n",
    "left_list_onehot_padded = pad(one_hot(left_list))\n",
    "right_list_onehot_padded = pad(one_hot(right_list))\n",
    "\n",
    "list_combined = []\n",
    "for l,f in zip(left_list_onehot_padded, right_list_onehot_padded): list_combined.append(l+f)\n",
    "\n",
    "torch.save(torch.FloatTensor(list_combined), \"data/metalation_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dd04fe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.load(\"data/metalation_motifs.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9256ae36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([600, 136])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834dc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046454a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e0d94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "motifs_pd = pd.read_csv(\"raw_data/metadata_600.csv\")\n",
    "\n",
    "# set up directory of how each symbol connects to ACGT: http://www.hgmd.cf.ac.uk/docs/nuc_lett.html\n",
    "# mapping 'N': [1,1,1,1] because it should accept any base target. Could be [0,0,0,0] because this is just padding...\n",
    "bas4_dict = {\n",
    "    'A': [1,0,0,0], 'C': [0,1,0,0], 'G': [0,0,1,0], 'T': [0,0,0,1],\n",
    "    'R': [1,0,1,0], 'Y': [0,1,0,1], 'K': [0,0,1,1], 'M': [1,1,0,0], 'S': [0,1,1,0], 'W': [1,0,0,1],\n",
    "    'B': [0,1,1,1], 'D': [1,0,1,1], 'H': [1,1,0,1], 'V': [1,1,1,0], 'N': [1,1,1,1] \n",
    "}\n",
    "\n",
    "def str_to_base4code(string):\n",
    "    coded = []\n",
    "    for char in string:\n",
    "        coded = coded + bas4_dict[char]\n",
    "    return coded\n",
    "\n",
    "# functions for building list of encoded strings and padding the lists\n",
    "def one_hot(data_1d):\n",
    "    data_1d_encoded = []\n",
    "    for string in data_1d:\n",
    "        data_1d_encoded.append(str_to_base4code(string))\n",
    "    return data_1d_encoded\n",
    "\n",
    "def pad(data_1d):\n",
    "    max_length = len(max(data_1d, key=len))\n",
    "    padded_data = []\n",
    "    for i, line in enumerate(data_1d):\n",
    "        padded_data.append(line + [0]*(max_length - len(line)))\n",
    "    return padded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cadab931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GATC  has no slash 1\n",
      "TCGA  has no slash 1\n",
      "TCGA  has no slash 2\n",
      "TTAA  has no slash 2\n",
      "GATC  has no slash 2\n",
      "RTAY  has no slash 2\n",
      "RTAY  has no slash 2\n",
      "RTAY  has no slash 2\n",
      "TTAA  has no slash 2\n",
      "TCGA  has no slash 2\n",
      "Ns are same on both sides?  True\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###### transitioning from pandas dataframe to list of the feature we want\n",
    "# 600: Do they want to keep their own testing data away from us for when we give them the model?\n",
    "motif11 = []#first half, left of /\n",
    "motif12 = []#first half, right of /\n",
    "for line in motifs_pd['Motif_1stHalf'].to_list():\n",
    "    motif11.append(line.rsplit('/')[0])\n",
    "    try:\n",
    "        motif12.append(line.rsplit('/')[1])\n",
    "    except:\n",
    "        motif12.append('')\n",
    "        print(line, \" has no slash 1\")\n",
    "motif21 = []\n",
    "motif22 = []\n",
    "\n",
    "for line in motifs_pd['Motif_2ndHalf'].to_list():\n",
    "    motif21.append(line.rsplit('/')[0])\n",
    "    try: # sometimes there is no / present\n",
    "        motif22.append(line.rsplit('/')[1])\n",
    "    except:\n",
    "        motif22.append('')\n",
    "        print(line, \" has no slash 2\")\n",
    "###### encoding and then padding each list sequences\n",
    "motif11 = one_hot(motif11)\n",
    "motif11 = pad(motif11)\n",
    "motif12 = one_hot(motif12)\n",
    "motif12 = pad(motif12)\n",
    "motif21 = one_hot(motif21)\n",
    "motif21 = pad(motif21)\n",
    "motif22 = one_hot(motif22)\n",
    "motif22 = pad(motif22)\n",
    "###### one hot encode the number of Ns for the spacing\n",
    "# compare 'N's on left / right half. double check they are even\n",
    "same_on_both_sides = True\n",
    "for line in motifs_pd['Methylation Motif']:\n",
    "    try:\n",
    "        a,b = line.split(\"/\")\n",
    "        if int(a.count('N')) != int(b.count('N')):\n",
    "            same_on_both_sides = False\n",
    "    except:\n",
    "        pass\n",
    "print(\"Ns are same on both sides? \", same_on_both_sides)\n",
    "\n",
    "# Get number of Ns into a one hot\n",
    "num_ns = []\n",
    "for line in motifs_pd['Methylation Motif']:\n",
    "    num_ns.append(int(line.count('N')/2))\n",
    "\n",
    "num_ns_onehot = []\n",
    "for n in num_ns:\n",
    "    num_ns_onehot.append([int(i) for i in np.eye(9)[n-2].tolist()])\n",
    "print(len(num_ns))\n",
    "######\n",
    "# bring together one hot version of motif 1st half part 1, motif 1st half part 2, motif 2nd half part 1, motif 2nd half part 2, and number of spaces (n) \n",
    "motifs_onehot = []\n",
    "for a, b, c, d, n in zip(motif11, motif12, motif21, motif22, num_ns_onehot):\n",
    "    motifs_onehot.append(a + b + c + d + n)\n",
    "\n",
    "torch.save(torch.FloatTensor(motifs_onehot), \"data/motifs-base4-numN.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23e3529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6b377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bba3cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f5379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58225f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6393927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4bd6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "protien_names = {'m':[],'r':[],'s':[],'target':[]}\n",
    "for seq_record in SeqIO.parse(\"raw_data/Type_I_M_subunit_genes_Protein_g_clean_sorted.faa\", \"fasta\"):\n",
    "    protien_names['m'].append(seq_record.id)\n",
    "for seq_record in SeqIO.parse(\"raw_data/Type_I_R_subunit_genes_Protein_g_clean_sorted.faa\", \"fasta\"):\n",
    "    protien_names['r'].append(seq_record.id)\n",
    "for seq_record in SeqIO.parse(\"raw_data/Type_I_S_subunit_genes_Protein_g_clean_sorted.faa\", \"fasta\"):\n",
    "    protien_names['s'].append(seq_record.id)\n",
    "t = pd.read_csv(\"raw_data/metadata_600.csv\")\n",
    "for _, row in t.iterrows():\n",
    "    protien_names['target'].append(row['RMS_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cd3d316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aba78I Aba78I Aba78I Aba7804I\n",
      "M.Aba78I Aba78IP S.Aba78I Aba7804IP\n",
      "\n",
      "Aba7804I Aba7804I Aba7804I Aba78I\n",
      "M.Aba7804I Aba7804IP S.Aba7804I Aba78IP\n",
      "\n",
      "Awo1030II Awo1030II Awo1030II Awo1030III\n",
      "M.Awo1030II Awo1030IIP S.Awo1030II Awo1030IIIP\n",
      "\n",
      "Awo1030III Awo1030III Awo1030III Awo1030II\n",
      "M.Awo1030III Awo1030IIIP S.Awo1030III Awo1030IIP\n",
      "\n",
      "BceSVI BceSVI BceSVI BceS28I\n",
      "M.BceSVI BceSVIP S.BceSVI BceS28IP\n",
      "\n",
      "BceS28I BceS28I BceS28I BceSVI\n",
      "M.BceS28I BceS28IP S.BceS28I BceSVIP\n",
      "\n",
      "BsuNI BsuNI BsuNI BsuN11I\n",
      "M.BsuNI BsuNIP S.BsuNI BsuN11IP\n",
      "\n",
      "BsuN11I BsuN11I BsuN11I BsuN22I\n",
      "M.BsuN11I BsuN11IP S.BsuN11I BsuN22IP\n",
      "\n",
      "BsuN22I BsuN22I BsuN22I BsuN31I\n",
      "M.BsuN22I BsuN22IP S.BsuN22I BsuN31IP\n",
      "\n",
      "BsuN31I BsuN31I BsuN31I BsuN42I\n",
      "M.BsuN31I BsuN31IP S.BsuN31I BsuN42IP\n",
      "\n",
      "BsuN42I BsuN42I BsuN42I BsuNI\n",
      "M.BsuN42I BsuN42IP S.BsuN42I BsuNIP\n",
      "\n",
      "CjeFII CjeFII CjeFII CjeF38011II\n",
      "M.CjeFII CjeFII S.CjeFII CjeF38011IIP\n",
      "\n",
      "CjeFIV CjeFIV CjeFIV CjeFII\n",
      "M.CjeFIV CjeFIV S.CjeFIV CjeFIIP\n",
      "\n",
      "CjeF38011II CjeF38011II CjeF38011II CjeFIV\n",
      "M.CjeF38011II CjeF38011IIP S.CjeF38011II CjeFIVP\n",
      "\n",
      "CsaI CsaI CsaI Csa8155I\n",
      "M.CsaI CsaIP S.CsaI Csa8155IP\n",
      "\n",
      "Csa8155I Csa8155I Csa8155I CsaI\n",
      "M.Csa8155I Csa8155IP S.Csa8155I CsaIP\n",
      "\n",
      "Eco06I Eco06I Eco06I Eco067II\n",
      "M.Eco06I Eco06IP S.Eco06I Eco067IIP\n",
      "\n",
      "Eco067II Eco067II Eco067II Eco06I\n",
      "M.Eco067II Eco067IIP S.Eco067II Eco06IP\n",
      "\n",
      "Eco137I Eco137I Eco137I Eco137II\n",
      "M.Eco137I Eco137IP S.Eco137I Eco137IIP\n",
      "\n",
      "Eco137II Eco137II Eco137II Eco137I\n",
      "M.Eco137II Eco137IIP S.Eco137II Eco137IP\n",
      "\n",
      "Eco2747I Eco2747I Eco2747I Eco2747II\n",
      "M.Eco2747I Eco2747IP S.Eco2747I Eco2747IIP\n",
      "\n",
      "Eco2747II Eco2747II Eco2747II Eco2747I\n",
      "M.Eco2747II Eco2747IIP S.Eco2747II Eco2747IP\n",
      "\n",
      "EcoEI EcoEI EcoEI EcoEC149I\n",
      "M.EcoEI EcoEI S.EcoEI EcoEC149IP\n",
      "\n",
      "EcoEC149I EcoEC149I EcoEC149I EcoEI\n",
      "M.EcoEC149I EcoEC149IP S.EcoEC149I EcoEIP\n",
      "\n",
      "EcoKI EcoKI EcoKI EcoK54I\n",
      "M.EcoKI EcoKI S.EcoKI EcoK54IP\n",
      "\n",
      "EcoK54I EcoK54I EcoK54I EcoKI\n",
      "M.EcoK54I EcoK54IP S.EcoK54I EcoKIP\n",
      "\n",
      "EcoMII EcoMII EcoMII EcoM9682I\n",
      "M.EcoMII EcoMII S.EcoMII EcoM9682IP\n",
      "\n",
      "EcoMIII EcoMIII EcoMIII EcoMC01I\n",
      "M.EcoMIII EcoMIIIP S.EcoMIII EcoMC01IP\n",
      "\n",
      "EcoM9682I EcoM9682I EcoM9682I EcoMIII\n",
      "M.EcoM9682I EcoM9682IP S.EcoM9682I EcoMIIIP\n",
      "\n",
      "EcoMC01I EcoMC01I EcoMC01I EcoMII\n",
      "M.EcoMC01I EcoMC01IP S.EcoMC01I EcoMIIP\n",
      "\n",
      "EcoNIH1I EcoNIH1I EcoNIH1I EcoNIH1III\n",
      "M.EcoNIH1I EcoNIH1IP S.EcoNIH1I EcoNIH1IIIP\n",
      "\n",
      "EcoNIH1III EcoNIH1III EcoNIH1III EcoNIH1I\n",
      "M.EcoNIH1III EcoNIH1IIIP S.EcoNIH1III EcoNIH1IP\n",
      "\n",
      "EcoR124I EcoR124I EcoR124I EcoR124II\n",
      "M.EcoR124I EcoR124I S.EcoR124I EcoR124IIP\n",
      "\n",
      "EcoR124II EcoR124II EcoR124II EcoR124I\n",
      "M.EcoR124II EcoR124II S.EcoR124II EcoR124IP\n",
      "\n",
      "Kpn142I Kpn142I Kpn142I Kpn1420II\n",
      "M.Kpn142I Kpn142IP S.Kpn142I Kpn1420IIP\n",
      "\n",
      "Kpn1420II Kpn1420II Kpn1420II Kpn142I\n",
      "M.Kpn1420II Kpn1420IIP S.Kpn1420II Kpn142IP\n",
      "\n",
      "PaePAI PaePAI PaePAI PaePA77I\n",
      "M.PaePAI PaePAIP S.PaePAI PaePA77IP\n",
      "\n",
      "PaePA77I PaePA77I PaePA77I PaePAI\n",
      "M.PaePA77I PaePA77IP S.PaePA77I PaePAIP\n",
      "\n",
      "PfrJSII PfrJSII PfrJSII PfrJS12aI\n",
      "M.PfrJSII PfrJSIIP S.PfrJSII PfrJS12aIP\n",
      "\n",
      "PfrJS12aI PfrJS12aI PfrJS12aI PfrJS21aI\n",
      "M.PfrJS12aI PfrJS12aIP S.PfrJS12aI PfrJS21aIP\n",
      "\n",
      "PfrJS21aI PfrJS21aI PfrJS21aI PfrJS25I\n",
      "M.PfrJS21aI PfrJS21aIP S.PfrJS21aI PfrJS25IP\n",
      "\n",
      "PfrJS25I PfrJS25I PfrJS25I PfrJS9aI\n",
      "M.PfrJS25I PfrJS25IP S.PfrJS25I PfrJS9aIP\n",
      "\n",
      "PfrJS9aI PfrJS9aI PfrJS9aI PfrJSII\n",
      "M.PfrJS9aI PfrJS9aIP S.PfrJS9aI PfrJSIIP\n",
      "\n",
      "Sen13I Sen13I Sen13I Sen13311II\n",
      "M.Sen13I Sen13IP S.Sen13I Sen13311IIP\n",
      "\n",
      "Sen13311II Sen13311II Sen13311II Sen13I\n",
      "M.Sen13311II Sen13311IIP S.Sen13311II Sen13IP\n",
      "\n",
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nClearly the issue is:\\n1. names have things pinned onto them. This has been managed.\\n2. the target data is out of order. \\n- Instead of zipping them, pick a protien name and try to find it in m,r,s,t\\n- If found then add each name variation into new dictionary. \\nprotien_names_use = {1:[\"M.Aba78I\",\"S.Aba78I\",\"Aba78IP\",\"Aba78IP\"], 2:[...]}\\n- Now I have a can loop through the dictionary indexs 1,2,3... to pull out the names in each respective files/csv \\nand pull in the data to do stuff with. This keeps it ordered and it only takes protiens present in the target.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the order / names all agree\n",
    "i = 0\n",
    "for m,r,s,t in zip(protien_names['m'],protien_names['r'],protien_names['s'],protien_names['target']):\n",
    "    mmod = m[2:]\n",
    "    if r[-1] == 'P':\n",
    "        rmod = r[:-1]\n",
    "    else:\n",
    "        rmod = r\n",
    "    if s.startswith('S1'):\n",
    "        smod = s[3:]\n",
    "    else:\n",
    "        smod = s[2:]\n",
    "    tmod = t[:-1]\n",
    "    if mmod != rmod or rmod != smod or smod != tmod: \n",
    "        print(mmod,rmod,smod,tmod)\n",
    "        print(m,r,s,t)\n",
    "        print()\n",
    "    i += 1\n",
    "print(i)\n",
    "'''\n",
    "Clearly the issue is:\n",
    "1. names have things pinned onto them. This has been managed.\n",
    "2. the target data is out of order. \n",
    "- Instead of zipping them, pick a protien name and try to find it in m,r,s,t\n",
    "- If found then add each name variation into new dictionary. \n",
    "protien_names_use = {1:[\"M.Aba78I\",\"S.Aba78I\",\"Aba78IP\",\"Aba78IP\"], 2:[...]}\n",
    "- Now I have a can loop through the dictionary indexs 1,2,3... to pull out the names in each respective files/csv \n",
    "and pull in the data to do stuff with. This keeps it ordered and it only takes protiens present in the target.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea5e4ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599])\n"
     ]
    }
   ],
   "source": [
    "# create method to find protien name in other dict elements. Can I find Aba78I in m,r,s,t ? This ignore sorting.\n",
    "name = \"Aba78I\"\n",
    "\n",
    "protien_names_ordered_filtered = {}\n",
    "for i, name_from_m in enumerate(protien_names['m']):\n",
    "    name = name_from_m[2:]\n",
    "    if (\"M.\" + name) in protien_names['m']:\n",
    "        if (name + \"P\") in protien_names['r']:\n",
    "            if (\"S.\" + name) in protien_names['s']:\n",
    "                if (name + \"P\") in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \"P\" + \".pt\", \"S.\" + name + \".pt\", name + \"P\"] # M. / P / S. / P\n",
    "                elif  name in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \"P\" + \".pt\", \"S.\" + name + \".pt\", name] # M. / P / S. / _\n",
    "            elif (\"S1.\" + name) in protien_names['s']:\n",
    "                if (name + \"P\") in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \"P\" + \".pt\", \"S1.\" + name + \".pt\", name + \"P\"] # M. / P / S1. / P\n",
    "                elif  name in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \"P\" + \".pt\", \"S1.\" + name + \".pt\", name] # M. / P / S1. / _\n",
    "        elif name in protien_names['r']:\n",
    "            if (\"S.\" + name) in protien_names['s']:\n",
    "                if (name + \"P\") in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \".pt\", \"S.\" + name + \".pt\", name + \"P\"] # M. / _ / S. / P\n",
    "                elif  name in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \".pt\", \"S.\" + name + \".pt\", name] # M. / _ / S. / _\n",
    "            elif (\"S1.\" + name) in protien_names['s']:\n",
    "                if (name + \"P\") in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \".pt\", \"S1.\" + name + \".pt\", name + \"P\"] # M. / _ / S1. / P\n",
    "                elif  name in protien_names['target']:\n",
    "                    protien_names_ordered_filtered[i] = [\"M.\" + name + \".pt\", name + \".pt\", \"S1.\" + name + \".pt\", name] # M. / _ / S1. / _\n",
    "\n",
    "print(protien_names_ordered_filtered.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6990a666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIRS_1 = [ \"temp_data/m1split/\", \"temp_data/r1split/\", \"temp_data/s1split/\"]\n",
    "\n",
    "INPUT_DIRS_2 = [ \"temp_data/m2split/\", \"temp_data/r2split/\", \"temp_data/s2split/\" ]\n",
    "\n",
    "def load_tensor(directory):\n",
    "    tensor_dict = torch.load(directory)\n",
    "    tensor_2d = tensor_dict['representations'][33]\n",
    "    tensor_flat = torch.flatten(tensor_2d)\n",
    "    return tensor_flat\n",
    "\n",
    "\n",
    "list_combined_1d_tensors = []\n",
    "for i in range(600):\n",
    "    hold_flat_tensor_single_protien = [] # will store the 1d tensor from m (m1 and maybe m2), then r, then s. \n",
    "    for mrst_index, (dir1, dir2) in enumerate(zip(INPUT_DIRS_1, INPUT_DIRS_2)):\n",
    "        tensor_1 = load_tensor(dir1 + protien_names_ordered_filtered[i][mrst_index])\n",
    "        if os.path.isfile(dir2 + protien_names_ordered_filtered[i][mrst_index]): # if this seq was too large it will have a tensor (probably smaller) in the second file\n",
    "            tensor_2 = load_tensor(dir2 + protien_names_ordered_filtered[i][mrst_index])\n",
    "            combined_tensor = torch.cat( (tensor_1, tensor_2) )\n",
    "        else:\n",
    "            combined_tensor = tensor_1 # If there is no tensor in the second file then we only need to load the first tensor\n",
    "        hold_flat_tensor_single_protien.append(combined_tensor)\n",
    "    list_combined_1d_tensors.append( torch.cat( ( hold_flat_tensor_single_protien[0], hold_flat_tensor_single_protien[1], hold_flat_tensor_single_protien[2] ) ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e953372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_val 3462400\n"
     ]
    }
   ],
   "source": [
    "#### get padding size\n",
    "len_list = []\n",
    "for list in list_combined_1d_tensors:\n",
    "    len_list.append(len(list))\n",
    "max_seq_len = max(len_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b8bb9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PAD\n",
    "\n",
    "list_combined_1d_tensors_padded = []\n",
    "for i, line in enumerate(list_combined_1d_tensors):\n",
    "    #list_combined_1d_tensors_padded.append( F.pad(list_combined_1d_tensors[i], ( max_len_val-len(line), 0), \"constant\" ) )\n",
    "    list_combined_1d_tensors_padded.append( F.pad(list_combined_1d_tensors[i], ( 0, max_len_val-len(line)), \"constant\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8cf1c7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3462400])\n",
      "torch.Size([2630400])\n",
      "tensor([0.1734, 0.1142, 0.0800,  ..., 0.0000, 0.0000, 0.0000])\n",
      "tensor([ 0.1734,  0.1142,  0.0800,  ...,  0.2647, -0.2722,  0.2444])\n"
     ]
    }
   ],
   "source": [
    "print(list_combined_1d_tensors_padded[0].size())\n",
    "print(list_combined_1d_tensors[0].size())\n",
    "print(list_combined_1d_tensors_padded[0])\n",
    "print(list_combined_1d_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "71d9b5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch stacking the list of tensors into single tensor\n",
      "data.size() torch.Size([600, 3462400])\n"
     ]
    }
   ],
   "source": [
    "#### build list of tensors into single torch tensor\n",
    "print(\"torch stacking the list of tensors into single tensor\")\n",
    "input_data = torch.stack(list_combined_1d_tensors_padded)\n",
    "\n",
    "print(\"data.size()\", input_data.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c66886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('saving')\n",
    "#torch.save(data, \"data/msr-esmb1-flat.pt\")\n",
    "if BUILD_PARTIAL: torch.save(data[0:2], \"data/PARTIALDATAmsr-esmb1-flat.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411c6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "89bb2edc013244411cbf7717b4d79309c84a79783ea40f60c98be42a1849b8fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
